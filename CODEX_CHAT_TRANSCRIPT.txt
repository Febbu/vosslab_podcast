Codex Project Transcript
Date: 2026-02-20
Location: /Users/pierre/job_campus/vosslab_podcast

Project Goal
- Build an automated weekly "podcast-style" summary of GitHub activity for user `vosslab`.
- Start simple: public repos only, last 7 days, repo-level activity only.
- Exclude forks for reporting.
- Later: generate narrated audio with Qwen3-TTS using multiple voices.

User Requirements Confirmed
- Weekly window: always last 7 days.
- Scope phase 1: original repos only (`fork=false`), repo-level updates only.
- Output should be automation-friendly and easy to extend.
- Use GitHub Actions for unattended weekly runs.
- English-only TTS initially.
- Keep host/analyst voices consistent each week; allow guest voice override sometimes.
- Use a separate Python environment for TTS; Conda chosen.

What Was Verified During Chat
- Repo `darrenoakey/tts` was checked and not identified as an Apple built-in TTS podcast-releases tool.
- User provided API JSON for `sort=created` and `sort=pushed`.
- Weekly digest for Feb 10-17, 2026 (UTC) was derived from provided data.

Files Created/Updated in This Project
- fetch_and_script.py
  - Fetches GitHub repos via API (created + pushed).
  - Filters last N days (default 7), excludes forks.
  - Writes `out/digest.json` and `out/script.txt`.
- .github/workflows/weekly.yml
  - Scheduled weekly run (Monday 09:00 UTC) + manual dispatch.
  - Uploads `out/` as workflow artifact.
- requirements.txt
  - Python dependency for digest script (`requests`).
- .gitignore
  - Ignores `.venv/`, `out/`, `__pycache__/`.
- README.md
  - Setup and usage for digest + TTS flow.
- tts_generate.py
  - Reads `out/script.txt` role lines (`HOST:`, `ANALYST:`, `GUEST:`).
  - Uses Qwen3-TTS CustomVoice model to synthesize per-role audio.
  - Writes `out/episode.wav`.
- voices.json
  - Stores stable role voices and optional guest override.

Local Test Results
- Digest script local run succeeded on user machine:
  - `out/digest.json` created.
  - `out/script.txt` created.
- Sandbox test initially failed due to network restrictions for pip in this environment.
- User local environment had internet and installed dependencies successfully.

Current Voice Config
- voices.json defaults:
  - host_voice: speaker_1
  - analyst_voice: speaker_2
  - guest_voice: speaker_3
  - guest_voice_override: null

Current Status
- Weekly text digest automation: implemented.
- Weekly publishing: NOT implemented.
- TTS script: implemented, needs local model/runtime setup and test run.
- MP3 conversion: not yet wired (current output is WAV).

How To Continue Next Session
1. Run weekly digest locally:
   - `python fetch_and_script.py`
2. Set up Conda TTS env and run audio generation:
   - `conda create -n qwen3-tts python=3.12 -y`
   - `conda activate qwen3-tts`
   - `pip install -U qwen-tts soundfile`
   - `python tts_generate.py`
3. Validate `out/episode.wav` quality and role voice mapping.
4. Add ffmpeg step to produce `out/episode.mp3`.
5. Add publishing step (optional): storage/feed/podcast host.

Notes for Future Codex
- User prefers concise, direct implementation steps.
- Keep solution pragmatic and incremental.
- Maintain this file as a running context log after each major session.

---
Session Update: 2026-02-20 (Codex Context Hardening)

What was actively done
- Added persistent transcript file:
  - `CODEX_CHAT_TRANSCRIPT.txt`
- Added project-level agent instructions:
  - `AGENTS.md`
- Verified transcript exists and contains prior project decisions and implementation status.

Why this was added
- Ensure future Codex sessions in this folder can recover context quickly.
- Reduce repeated setup/explanation in each new session.

Current project state
- Weekly digest automation exists (`fetch_and_script.py` + GitHub Actions schedule).
- Local digest test previously succeeded (`out/digest.json`, `out/script.txt`).
- TTS script exists (`tts_generate.py`) with role voice config (`voices.json`).
- Publishing step is still not implemented.

Immediate next steps
1. Run TTS environment setup (Conda) and execute `tts_generate.py`.
2. Validate generated audio quality and role-to-voice mapping.
3. Add optional WAV->MP3 conversion (ffmpeg).
4. Decide publishing target (artifact only vs hosted feed).

Instruction for future sessions
- Start by reading this transcript and `AGENTS.md`.
- Append a dated "Session Update" block after major progress.

---
Session Update: 2026-02-21 (Repo Initialization + GitHub Publish)

What changed
- Updated `.gitignore` to exclude `.venv_tts/` in addition to existing ignores.
- Created initial git commit on `main` with current project files.
- Created and published a new GitHub repository:
  - `https://github.com/Febbu/vosslab_podcast`
- Added remote `origin` and pushed `main`.

What was tested
- Verified git commit creation succeeded.
- Verified GitHub repo creation and first push succeeded.
- Verified local `main` tracks `origin/main`.

Next actions
1. Implement modular daily pipeline (`logs -> outline -> blog -> script -> audio`) as separate scripts.
2. Add daily GitHub Actions workflow for artifact chaining.
3. Integrate external TTS wrapper (`darrenoakey/tts`) in the final audio step.

---
Session Update: 2026-02-22 (Weekly Commit Messages in Digest)

What changed
- Updated `fetch_and_script.py` to fetch commit history for each updated repo within the active 7-day window.
- Added `weekly_commit_messages` to `out/digest.json`, including repo name, full name, up to 5 unique commit subject lines, and count.
- Updated script rendering to include a short "Weekly commit highlights" line in `out/script.txt` using recent commit subjects.

What was tested
- `python3 -m py_compile fetch_and_script.py` (pass).
- `python3 fetch_and_script.py` in this shell failed due to missing dependency: `ModuleNotFoundError: No module named 'requests'`.

Next actions
1. Install dependencies (`pip install -r requirements.txt`) in the runtime environment.
2. Re-run `python3 fetch_and_script.py` to validate API fetches and output formatting.
3. Confirm commit highlight phrasing is suitable for narration and adjust limits if needed.

---
Session Update: 2026-02-22 (Pipeline Redesign + Manager Plan)

What changed
- Updated `README.md` to document the new target process flow:
  1. `fetch_github_data.py`
  2. `outline_github_data.py`
  3. `outline_to_blog_post.py`, `outline_to_bluesky_post.py`, `outline_to_podcast_script.py`
  4. `script_to_audio.py`
- Added planned output artifacts and constraints in `README.md` (500-word blog, 140-char Bluesky, 500-word N-speaker script).
- Created planning directories `docs/active_plans/` and `docs/archive/`.
- Added manager-grade implementation plan:
  - `docs/active_plans/GITHUB_CONTENT_PIPELINE_MILESTONE_PLAN.md`
  - Includes milestones, workstreams, dependency IDs, gates, migration policy, risk register, patch plan, and open decisions.

What was tested
- Verified `README.md` contains the new pipeline and output targets.
- Verified plan document exists and includes required sections per skill guidance.
- Noted planning context gaps in repo: `refactor_progress.md` missing and no prior plan history to reference.

Next actions
1. Approve open decisions in the plan (schema depth, canonical outline format, overflow policy, default `NUM_SPEAKERS`, audio format targets).
2. Start implementation from Milestone M1 (`fetch_github_data.py` + JSONL contract + validation gates).
3. Add `refactor_progress.md` so future plan updates can track active/completed/pending status cleanly.

---
Session Update: 2026-02-22 (Pipeline Implemented End-to-End)

What changed
- Implemented staged pipeline scripts:
  - `fetch_github_data.py`
  - `outline_github_data.py`
  - `outline_to_blog_post.py`
  - `outline_to_bluesky_post.py`
  - `outline_to_podcast_script.py`
  - `script_to_audio.py`
- Added shared text-limit helper module:
  - `pipeline_text_utils.py`
- Added targeted tests for parser aggregation and output-limit behavior:
  - `tests/test_outline_parser.py`
  - `tests/test_content_pipeline_limits.py`
- Updated workflow and docs to run/document the new staged flow:
  - `.github/workflows/weekly.yml`
  - `README.md`
  - `docs/CHANGELOG.md`

What was tested
- `python3 -m py_compile fetch_github_data.py outline_github_data.py outline_to_blog_post.py outline_to_bluesky_post.py outline_to_podcast_script.py script_to_audio.py pipeline_text_utils.py`
- `python3 -m py_compile tests/test_outline_parser.py tests/test_content_pipeline_limits.py`
- `.venv/bin/python fetch_github_data.py --help` (pass)
- `.venv/bin/python -m pytest -q tests/test_outline_parser.py tests/test_content_pipeline_limits.py` (pass: `5 passed`)
- Live fetch smoke:
  - `.venv/bin/python fetch_github_data.py --user vosslab --window-days 7 --max-repos 1 --output out/smoke_github_data.jsonl`
- Smoke pipeline on sample JSONL:
  - `python3 outline_github_data.py --input out/sample_github_data.jsonl --outline-json out/sample_outline.json --outline-txt out/sample_outline.txt`
  - `python3 outline_to_blog_post.py --input out/sample_outline.json --output out/sample_blog_post.html --word-limit 500`
  - `python3 outline_to_bluesky_post.py --input out/sample_outline.json --output out/sample_bluesky_post.txt --char-limit 140`
  - `python3 outline_to_podcast_script.py --input out/sample_outline.json --output out/sample_podcast_script.txt --num-speakers 3 --word-limit 500`
- Smoke pipeline on live-fetch output:
  - `.venv/bin/python outline_github_data.py --input out/smoke_github_data.jsonl --outline-json out/smoke_outline.json --outline-txt out/smoke_outline.txt`
  - `.venv/bin/python outline_to_blog_post.py --input out/smoke_outline.json --output out/smoke_blog_post.html --word-limit 500`
  - `.venv/bin/python outline_to_bluesky_post.py --input out/smoke_outline.json --output out/smoke_bluesky_post.txt --char-limit 140`
  - `.venv/bin/python outline_to_podcast_script.py --input out/smoke_outline.json --output out/smoke_podcast_script.txt --num-speakers 3 --word-limit 500`

Next actions
1. Run full weekly fetch without `--max-repos` to generate production-size `out/github_data.jsonl`.
2. Install audio dependencies (`numpy`, `soundfile`, `torch`, `qwen_tts`) in TTS environment and run `script_to_audio.py`.
3. Decide when to retire legacy scripts (`fetch_and_script.py`, `tts_generate.py`) after final signoff.

---
Session Update: 2026-02-22 (Fetch Stage Window Presets + Daily Cache + Changelog Pull)

What changed
- Updated `fetch_github_data.py` to support exclusive time-window presets:
  - `--last-day` (default)
  - `--last-two-days`
  - `--last-week`
  - `--last-month`
  - `--window-days N` for custom windows
- Added relevant-repo changelog ingestion in `fetch_github_data.py`:
  - Attempts to fetch `docs/CHANGELOG.md` from each relevant repo.
  - Emits `repo_changelog` records with latest dated entry extraction.
- Added per-day cache output in `fetch_github_data.py`:
  - Writes one JSONL per day in the active window to `--daily-cache-dir` (default `out/daily_cache`).
- Updated workflow and docs:
  - `.github/workflows/weekly.yml` now uses `--last-week` and writes `out/daily_cache`.
  - `README.md` now documents window presets, changelog records, and daily cache behavior.
- Added targeted tests:
  - `tests/test_fetch_github_data_features.py`

What was tested
- `.venv/bin/python -m py_compile fetch_github_data.py tests/test_fetch_github_data_features.py` (pass)
- `.venv/bin/python fetch_github_data.py --help` (pass, new flags visible)
- `.venv/bin/python -m pytest -q tests/test_fetch_github_data_features.py` (pass: `4 passed`)
- Live fetch smoke with day-cache output:
  - `.venv/bin/python fetch_github_data.py --user vosslab --last-week --max-repos 1 --output out/smoke_github_data.jsonl --daily-cache-dir out/smoke_daily_cache`
  - Result: main JSONL written plus 7 daily cache files.
- Downstream smoke remained functional:
  - `.venv/bin/python outline_github_data.py --input out/smoke_github_data.jsonl --outline-json out/smoke_outline2.json --outline-txt out/smoke_outline2.txt`
  - `.venv/bin/python outline_to_blog_post.py --input out/smoke_outline2.json --output out/smoke_blog_post2.html --word-limit 500`

Next actions
1. Run full fetch without `--max-repos` to populate production `out/github_data.jsonl` and daily cache files.
2. Decide whether changelog records should include only latest entry (current behavior) or full file text.
3. Run full staged pipeline and then validate `script_to_audio.py` in TTS environment.

---
Session Update: 2026-02-22 (Repo Structure Cleanup + Repo-Sharded Outlines)

What changed
- Reorganized code to reduce root clutter:
  - Pipeline scripts moved under `pipeline/`
  - Local scheduling scripts moved under `automation/`
- Added local macOS automation helpers:
  - `automation/run_local_pipeline.sh`
  - `automation/install_launchd_pipeline.sh`
  - `automation/uninstall_launchd_pipeline.sh`
- Updated references in docs/workflow/tests to new paths under `pipeline/`.
- Enhanced `pipeline/outline_github_data.py` with per-repo sharding:
  - Writes one JSON shard and one text shard per repo.
  - Writes manifest index at `out/outline_repos/index.json`.
  - Added flags: `--repo-shards-dir` and `--skip-repo-shards`.
- Added parser coverage for shard output:
  - `tests/test_outline_parser.py` now validates shard file generation and manifest contents.

What was tested
- `.venv/bin/python -m py_compile pipeline/fetch_github_data.py pipeline/outline_github_data.py pipeline/outline_to_blog_post.py pipeline/outline_to_bluesky_post.py pipeline/outline_to_podcast_script.py pipeline/script_to_audio.py pipeline/pipeline_text_utils.py tests/test_outline_parser.py tests/test_content_pipeline_limits.py tests/test_fetch_github_data_features.py` (pass)
- `.venv/bin/python -m pytest -q tests/test_outline_parser.py tests/test_content_pipeline_limits.py tests/test_fetch_github_data_features.py` (pass: `9 passed`)
- `.venv/bin/python pipeline/outline_github_data.py --input out/smoke_github_data.jsonl --outline-json out/smoke_outline3.json --outline-txt out/smoke_outline3.txt --repo-shards-dir out/smoke_outline_repos` (pass)
- Shell syntax checks:
  - `bash -n automation/run_local_pipeline.sh automation/install_launchd_pipeline.sh automation/uninstall_launchd_pipeline.sh` (pass)

Next actions
1. Run full fetch and outline generation to populate repo shard files with real multi-repo data.
2. Update downstream summarization stage to consume `out/outline_repos/index.json` and summarize per repo.
3. Validate local `launchd` install/uninstall flow on macOS user session.

---
Session Update: 2026-02-22 (LLM Summarizer Wired into outline_github_data.py)

What changed
- Updated `pipeline/outline_github_data.py` to use `local-llm-wrapper` for outline summarization.
- Added LLM-generated fields to output:
  - `llm_repo_outline` per repo in `repo_activity`
  - `llm_global_outline` for weekly cross-repo summary
- Added CLI controls:
  - `--llm-transport` (`ollama`, `apple`, `auto`)
  - `--llm-model`
  - `--llm-max-tokens`
  - `--llm-repo-limit`
  - `--disable-llm` (deterministic fallback mode)
- Kept deterministic aggregation as data backbone, but default mode now runs LLM summarization.
- Added test coverage for LLM summarization function wiring in `tests/test_outline_parser.py`.
- Updated README and changelog to document the LLM outline behavior.

What was tested
- `.venv/bin/python -m py_compile pipeline/outline_github_data.py tests/test_outline_parser.py` (pass)
- `.venv/bin/python -m pytest -q tests/test_outline_parser.py` (pass: `4 passed`)
- `.venv/bin/python pipeline/outline_github_data.py --help` (pass; LLM flags visible)

Next actions
1. Run `pipeline/outline_github_data.py` against your real weekly JSONL with `--llm-transport ollama`.
2. Verify Ollama model choice and tune `--llm-max-tokens` / `--llm-repo-limit` for latency.
3. Optionally wire blog/bluesky/podcast scripts to consume `llm_global_outline` and `llm_repo_outline`.

---
Session Update: 2026-02-22 (Dependency Manifest + Brewfile for Pipeline and LLM Wrapper)

What changed
- Updated `pip_requirements.txt` to reflect core pipeline runtime dependencies and include notes
  about local-llm-wrapper runtime expectations.
- Added root `Brewfile` for macOS system dependencies used by pipeline + local LLM runtime:
  - `python@3.12`
  - `ollama`
  - `libsndfile`
  - `ffmpeg`
- Updated `README.md` local setup to use:
  - `pip install -r pip_requirements.txt`
  - `brew bundle --file Brewfile`
- Updated `automation/run_local_pipeline.sh` setup hint to reference `pip_requirements.txt`.

What was tested
- `python3 tests/check_ascii_compliance.py -i pip_requirements.txt` (pass)
- `python3 tests/check_ascii_compliance.py -i Brewfile` (pass)
- `python3 tests/check_ascii_compliance.py -i README.md` (pass)
- Existing parser test gate remained green:
  - `.venv/bin/python -m pytest -q tests/test_outline_parser.py` (pass: `4 passed`)

Next actions
1. Run `brew bundle --file Brewfile` on macOS host to install system packages.
2. Recreate/refresh `.venv` from `pip_requirements.txt` and run end-to-end local pipeline.
3. Verify local Ollama model availability for `pipeline/outline_github_data.py --llm-transport ollama`.

---
Session Update: 2026-02-22 (apple-foundation-models Required)

What changed
- Updated `pip_requirements.txt` to make `apple-foundation-models` a required dependency.
- Updated `README.md` LLM section to state `apple-foundation-models` is required.
- Updated `docs/CHANGELOG.md` to record the new dependency requirement.

What was tested
- `python3 tests/check_ascii_compliance.py -i pip_requirements.txt` (pass)
- `python3 tests/check_ascii_compliance.py -i README.md` (pass)
- `python3 tests/check_ascii_compliance.py -i docs/CHANGELOG.md` (pass)

Next actions
1. Reinstall environment dependencies from `pip_requirements.txt`.
2. Validate Apple transport on target macOS host:
   - `pipeline/outline_github_data.py --llm-transport apple`
3. Keep Ollama path available as fallback when Apple runtime is unavailable.

---
Session Update: 2026-02-22 (LLM-Only Outline Mode)

What changed
- Updated `pipeline/outline_github_data.py` to make LLM summarization mandatory on every run.
- Removed deterministic-only toggle argument `--disable-llm`.
- Updated `README.md` wording to remove "by default" and describe LLM summarization as the only mode.
- Updated `docs/CHANGELOG.md` with the LLM-only mode change.

What was tested
- `.venv/bin/python pipeline/outline_github_data.py --help` (pass; `--disable-llm` no longer shown)
- `.venv/bin/python -m py_compile pipeline/outline_github_data.py` (pass)
- `.venv/bin/python -m pytest -q tests/test_outline_parser.py` (pass: `4 passed`)

Next actions
1. Run real weekly outline generation with Ollama:
   - `pipeline/outline_github_data.py --llm-transport ollama`
2. Validate Apple path on compatible macOS host:
   - `pipeline/outline_github_data.py --llm-transport apple`
3. Tune `--llm-max-tokens` and `--llm-repo-limit` for runtime/cost constraints.

---
Session Update: 2026-02-22 (settings.yaml + PyYAML Defaults for User and LLM)

What changed
- Added required dependency `pyyaml` to `pip_requirements.txt`.
- Added root `settings.yaml` with:
  - `github.username`
  - `llm.transport`, `llm.model`, `llm.max_tokens`, `llm.repo_limit`
- Added `pipeline/pipeline_settings.py` for shared YAML settings loading/resolution helpers.
- Updated `pipeline/fetch_github_data.py`:
  - New `--settings` flag (default `settings.yaml`)
  - Default GitHub username now reads from `settings.yaml` and falls back to `vosslab`
  - CLI `--user` still overrides settings
- Updated `pipeline/outline_github_data.py`:
  - New `--settings` flag (default `settings.yaml`)
  - Default LLM transport/model/max_tokens/repo_limit now read from `settings.yaml`
  - CLI LLM flags still override settings
- Updated `automation/run_local_pipeline.sh` to pass `--settings settings.yaml` for fetch + outline.
- Updated `README.md` with settings file documentation and override behavior.
- Added `tests/test_pipeline_settings.py`.

What was tested
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/pipeline_settings.py pipeline/fetch_github_data.py pipeline/outline_github_data.py tests/test_pipeline_settings.py`
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_pipeline_settings.py tests/test_fetch_github_data_features.py tests/test_outline_parser.py` (pass: `11 passed`)

Next actions
1. Add settings support for additional pipeline defaults (for example output paths and speaker count) only if they are repeatedly changed by hand.
2. Decide whether `settings.yaml` should include a weekly/default window preset for fetch stage automation.
3. Continue verbosity pass across all scripts so each stage logs intent and progress consistently.

---
Session Update: 2026-02-22 (fetch_github_data.py migrated to PyGithub adapter)

What changed
- Added `pipeline/github_client.py` with a `GitHubClient` adapter class and `RateLimitError`.
- Migrated `pipeline/fetch_github_data.py` from direct `requests` API calls to adapter-backed
  PyGithub calls for:
  - user repos
  - repo commits in window
  - repo issues/PRs updated since window start
  - `docs/CHANGELOG.md` file reads from default branch
- Kept CLI surface and output schema stable:
  - Existing flags remain
  - Existing JSONL `record_type` values and record keys remain
- Added serializer helpers in `pipeline/fetch_github_data.py` to convert PyGithub objects into
  REST-like dicts used by existing `build_*_record()` functions.
- Added `PyGithub` to `pip_requirements.txt`.
- Improved failure mode when PyGithub is not installed:
  - script logs a clear dependency message and exits cleanly without traceback.

What was tested
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/github_client.py pipeline/fetch_github_data.py`
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_fetch_github_data_features.py tests/test_pipeline_settings.py` (pass: `7 passed`)
- Smoke execution without PyGithub installed confirms clean messaging (no traceback):
  - `python3.12 pipeline/fetch_github_data.py --max-repos 1 ...`

Next actions
1. Install dependency locally: `pip install PyGithub` (or reinstall from `pip_requirements.txt`).
2. Re-run fetch smoke with `--max-repos 1` to verify live PyGithub API path in your environment.
3. Optionally add focused tests for serializer field parity against expected REST-like payload keys.

---
Session Update: 2026-02-22 (Explicit Progress Logging Across All Pipeline Scripts)

What changed
- Added explicit timestamped progress logs to all main pipeline stage scripts:
  - `pipeline/outline_github_data.py`
  - `pipeline/outline_to_blog_post.py`
  - `pipeline/outline_to_bluesky_post.py`
  - `pipeline/outline_to_podcast_script.py`
  - `pipeline/script_to_audio.py`
- Logging now reports stage start, major processing milestones, and final outputs for easier
  local observability and machine-diff friendly run traces.
- Added explicit timestamped progress logs to local automation shell scripts:
  - `automation/run_local_pipeline.sh`
  - `automation/install_launchd_pipeline.sh`
  - `automation/uninstall_launchd_pipeline.sh`

What was tested
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/outline_github_data.py pipeline/outline_to_blog_post.py pipeline/outline_to_bluesky_post.py pipeline/outline_to_podcast_script.py pipeline/script_to_audio.py`
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_outline_parser.py tests/test_content_pipeline_limits.py` (pass: `7 passed`)
- `bash -n automation/run_local_pipeline.sh automation/install_launchd_pipeline.sh automation/uninstall_launchd_pipeline.sh`

Next actions
1. Run a full local pipeline once to verify desired verbosity level across long runs.
2. Tune logging granularity if any stage is too noisy (for example per-line audio generation logs).
3. Add equivalent progress logging to any future new stage scripts at creation time.

---
Session Update: 2026-02-22 (Progress Log Timestamp Format Simplified)

What changed
- Updated all newly added progress log prefixes in pipeline and automation scripts to use
  time-only format `HH:MM:SS` instead of full ISO datetime with milliseconds/timezone.
- Example new format:
  - `[fetch_github_data 02:54:44]`
- Applied to:
  - `pipeline/fetch_github_data.py`
  - `pipeline/outline_github_data.py`
  - `pipeline/outline_to_blog_post.py`
  - `pipeline/outline_to_bluesky_post.py`
  - `pipeline/outline_to_podcast_script.py`
  - `pipeline/script_to_audio.py`
  - `automation/run_local_pipeline.sh`
  - `automation/install_launchd_pipeline.sh`
  - `automation/uninstall_launchd_pipeline.sh`

What was tested
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/fetch_github_data.py pipeline/outline_github_data.py pipeline/outline_to_blog_post.py pipeline/outline_to_bluesky_post.py pipeline/outline_to_podcast_script.py pipeline/script_to_audio.py`
- `bash -n automation/run_local_pipeline.sh automation/install_launchd_pipeline.sh automation/uninstall_launchd_pipeline.sh`

Next actions
1. Run one full local pipeline execution and confirm the new short log format is preferred.
2. If needed, trim any overly chatty per-item logs while retaining stage-level visibility.

---
Session Update: 2026-02-22 (Argparse Minimization in fetch_github_data.py)

What changed
- Simplified `pipeline/fetch_github_data.py` CLI for argument minimization:
  - Removed `--api-base`.
  - Removed `--token`.
  - Replaced window presets and `--window-days` with one option: `--last-days N` (default 1).
- Updated fetch auth source to use `settings.yaml` only:
  - optional token now read from `github.token`.
  - removed `GH_TOKEN`/`GITHUB_TOKEN` fallback logic from fetch script.
- Added `github.token` key to root `settings.yaml`.
- Updated `automation/run_local_pipeline.sh` fetch call to use `--last-days 7`.
- Updated `README.md` to reflect minimized fetch arguments and settings-based token configuration.
- Updated `tests/test_fetch_github_data_features.py` for new window argument behavior and validation.
- Removed `requests` from `pip_requirements.txt` (no longer used after PyGithub migration).
- Updated `pipeline/github_client.py` rate-limit guidance text to reference `settings.yaml`.

What was tested
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/fetch_github_data.py pipeline/github_client.py tests/test_fetch_github_data_features.py`
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_fetch_github_data_features.py tests/test_pipeline_settings.py tests/test_outline_parser.py tests/test_content_pipeline_limits.py` (pass: `15 passed`)
- `source source_me.sh && python3.12 pipeline/fetch_github_data.py --help`

Next actions
1. Set `github.token` in `settings.yaml` for higher rate limits when needed.
2. Run one local fetch smoke (`--max-repos 1`) after installing PyGithub in runtime environment.
3. Apply similar argparse minimization pass to other scripts only where users rarely change flags.

---
Session Update: 2026-02-22 (LLM Provider Flags in settings.yaml)

What changed
- Confirmed and kept shared YAML loader module: `pipeline/pipeline_settings.py`.
- Extended `pipeline/pipeline_settings.py` with provider-resolution helpers:
  - `get_setting_bool(...)`
  - `get_enabled_llm_transport(...)`
  - `get_llm_provider_model(...)`
- Updated `pipeline/outline_github_data.py` to use enabled-provider settings for default LLM
  transport and model selection.
- Added strict validation rule: if more than one LLM provider is enabled in `settings.yaml`,
  outline generation raises a clear runtime error.
- Updated root `settings.yaml` to provider-flag structure and set Apple as active local provider:
  - `llm.providers.apple.enabled: true`
  - `llm.providers.ollama.enabled: false`
- Updated `README.md` to document provider names and single-enabled-provider expectation.
- Expanded `tests/test_pipeline_settings.py` for enabled-provider and validation behavior.

What was tested
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/pipeline_settings.py pipeline/outline_github_data.py tests/test_pipeline_settings.py`
- `source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_pipeline_settings.py tests/test_outline_parser.py tests/test_content_pipeline_limits.py tests/test_fetch_github_data_features.py` (pass: `19 passed`)
- `source source_me.sh && python3.12 pipeline/outline_github_data.py --help`

Next actions
1. Validate one real outline run using Apple transport from settings to confirm local runtime.
2. If needed later, flip provider flags in `settings.yaml` to switch between Apple and Ollama.
3. Consider removing `--llm-transport` CLI override if you want strict settings-only provider selection.

---
Session Update: 2026-02-22 (Single-Speaker macOS say TTS Path Added)

What changed
- Added `pipeline/script_to_audio_say.py` as a dedicated macOS `say` backend for single-speaker
  output from `out/podcast_script.txt`.
- New script supports:
  - explicit progress logging with `HH:MM:SS` timestamps,
  - `--list-voices` to show installed macOS voices,
  - `--voice` override (including a `Siri` alias lookup),
  - `--rate-wpm` override,
  - settings fallback from `settings.yaml` (`tts.say.voice`, `tts.say.rate_wpm`),
  - collapsing multi-speaker script lines into one narration block for one-voice synthesis.
- Added tests: `tests/test_script_to_audio_say.py` for parser/narration/voice-resolution helpers.
- Updated docs/config:
  - `README.md` now documents both audio paths (`script_to_audio.py` and `script_to_audio_say.py`).
  - `settings.yaml` now includes `tts.say` defaults.
  - `docs/CHANGELOG.md` now records this addition.

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/script_to_audio_say.py tests/test_script_to_audio_say.py'`
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_script_to_audio_say.py'` (pass: `5 passed`)
- `bash -lc 'source source_me.sh && python3.12 pipeline/script_to_audio_say.py --help'`

Next actions
1. Run one real synthesis locally with `--voice Siri` and confirm desired voice/rate quality.
2. If specific Siri voice names differ on your machine, use `--list-voices` and set
   `settings.yaml` `tts.say.voice` to an exact installed name.
3. Optionally add a `run_local_pipeline.sh` branch to call `script_to_audio_say.py` when you want
   one-voice audio instead of Qwen multi-speaker output.

---
Session Update: 2026-02-22 (Fetch Window Presets Restored)

What changed
- Updated `pipeline/fetch_github_data.py` CLI window handling to use preset flags:
  - `--last-day` (default),
  - `--last-week`,
  - `--last-month`.
- Removed `--last-days` from fetch CLI.
- Updated `resolve_window_days()` to map presets to 1/7/30 days.
- Updated `automation/run_local_pipeline.sh` fetch call from `--last-days 7` to `--last-week`.
- Updated `README.md` fetch command and notes to document preset window flags.
- Updated `tests/test_fetch_github_data_features.py` for preset-based window resolution.

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/fetch_github_data.py tests/test_fetch_github_data_features.py'`
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_fetch_github_data_features.py'` (pass: `5 passed`)
- `bash -lc 'source source_me.sh && python3.12 pipeline/fetch_github_data.py --help'`

Next actions
1. Run fetch locally with one preset (for example `--last-week`) to verify expected window output.
2. If desired, add `--last-two-days` as an extra preset in a follow-up without reintroducing
   free-form day counts.

---
Session Update: 2026-02-22 (PyGithub Rate-Limit Compatibility Fix)

What changed
- Fixed `pipeline/github_client.py` rate-limit parsing to support multiple PyGithub shapes:
  - `get_rate_limit().core`
  - `get_rate_limit().resources.core`
  - `get_rate_limit().resources["core"]`
- Added defensive handling so proactive rate-limit checks do not crash when metadata shape is
  unexpected; fetch continues and still reports clear rate-limit errors on actual 403 responses.
- Added tests in `tests/test_github_client_rate_limit.py` to cover supported shapes and
  unknown-shape safety.

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/github_client.py tests/test_github_client_rate_limit.py'`
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_github_client_rate_limit.py tests/test_fetch_github_data_features.py'` (pass: `9 passed`)
- `bash -lc 'source source_me.sh && python3.12 pipeline/fetch_github_data.py --last-day --max-repos 1 --output out/smoke_github_data.jsonl --daily-cache-dir out/smoke_daily_cache'` (pass)

Next actions
1. Re-run fetch locally with the new preset flags (`--last-day`, `--last-week`, or `--last-month`).
2. Add `github.token` in `settings.yaml` for higher rate limits if you plan full-repo runs.

---
Session Update: 2026-02-22 (Outline Continue-Mode Cache Reuse)

What changed
- Updated `pipeline/outline_github_data.py` to support resume caching of repo outlines via:
  - `--continue` (enabled by default),
  - `--no-continue` (force full repo outline regeneration).
- Continue mode now loads cached repo shards from `--repo-shards-dir` and reuses
  `repo_activity.llm_repo_outline` when user/window metadata match current input.
- Summarization now tracks cache behavior in outline output:
  - `llm_cached_repo_outline_count`
  - `llm_generated_repo_outline_count`
- LLM client creation is now lazy in outline summarize flow and only initialized when needed.
- Updated `README.md` to document default continue behavior and `--no-continue`.
- Added cache-focused tests in `tests/test_outline_parser.py`.

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/outline_github_data.py tests/test_outline_parser.py'`
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_outline_parser.py'` (pass: `6 passed`)
- `bash -lc 'source source_me.sh && python3.12 pipeline/outline_github_data.py --help'`

Next actions
1. Re-run outline stage against existing `out/outline_repos` to confirm cache hits on repeat runs.
2. Use `--no-continue` only when forcing complete re-generation after prompt/schema changes.

---
Session Update: 2026-02-22 (Blog Stage Switched to LLM Markdown Output)

What changed
- Updated `pipeline/outline_to_blog_post.py` to generate blog content through `local-llm-wrapper`
  instead of deterministic paragraph assembly.
- Blog output is now Markdown by default (`out/blog_post.md`) so it can be used directly with
  MkDocs Material workflows.
- `--word-limit` behavior for blog stage now acts as a target length; if output exceeds target,
  the script logs the over-target result and keeps content instead of hard-failing.
- Added blog generation tests in `tests/test_outline_to_blog_post.py`.
- Updated `tests/test_content_pipeline_limits.py` to validate Markdown-oriented trim behavior.
- Updated `README.md` process/output wording from HTML blog output to Markdown blog output.

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/outline_to_blog_post.py tests/test_content_pipeline_limits.py tests/test_outline_to_blog_post.py'`
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_content_pipeline_limits.py tests/test_outline_to_blog_post.py'` (pass: `5 passed`)
- `bash -lc 'source source_me.sh && python3.12 pipeline/outline_to_blog_post.py --help'`

Next actions
1. Run one real blog generation against `out/outline.json` and inspect Markdown readability in
   your MkDocs context.
2. If needed, tune blog prompt wording in code before introducing shared prompt-file templates.

---
Session Update: 2026-02-22 (Podlib Import Path Migration)

What changed
- Updated pipeline and test imports to match utility module move into `pipeline/podlib/`.
- Replaced direct imports:
  - `import github_client` -> `from podlib import github_client`
  - `import pipeline_settings` -> `from podlib import pipeline_settings`
  - `import pipeline_text_utils` -> `from podlib import pipeline_text_utils`
- Applied updates to:
  - `pipeline/fetch_github_data.py`
  - `pipeline/outline_github_data.py`
  - `pipeline/outline_to_blog_post.py`
  - `pipeline/outline_to_bluesky_post.py`
  - `pipeline/outline_to_podcast_script.py`
  - `pipeline/script_to_audio_say.py`
  - `tests/test_pipeline_settings.py`
  - `tests/test_github_client_rate_limit.py`
  - `tests/test_content_pipeline_limits.py`
  - `tests/test_outline_to_blog_post.py`

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m py_compile pipeline/fetch_github_data.py pipeline/outline_github_data.py pipeline/outline_to_blog_post.py pipeline/outline_to_bluesky_post.py pipeline/outline_to_podcast_script.py pipeline/script_to_audio_say.py tests/test_pipeline_settings.py tests/test_github_client_rate_limit.py tests/test_content_pipeline_limits.py tests/test_outline_to_blog_post.py'`
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_pipeline_settings.py tests/test_github_client_rate_limit.py tests/test_content_pipeline_limits.py tests/test_outline_to_blog_post.py'` (pass: `19 passed`)
- `bash -lc 'source source_me.sh && python3.12 pipeline/fetch_github_data.py --help && python3.12 pipeline/outline_to_blog_post.py --help'`

Next actions
1. Keep new utility modules under `pipeline/podlib/` and route any future shared helpers there.
2. Optionally add `pipeline/podlib/__init__.py` if you want explicit package marker semantics.

---
Session Update: 2026-02-22 (First-Party local-llm-wrapper Hygiene Cleanup)

What changed
- Treated `local-llm-wrapper` as first-party code and fixed hygiene/test failures directly.
- Migrated `local_llm_wrapper` package imports from relative form to absolute package imports
  (`local_llm_wrapper.*`) across:
  - `local-llm-wrapper/local_llm_wrapper/llm.py`
  - `local-llm-wrapper/local_llm_wrapper/llm_client.py`
  - `local-llm-wrapper/local_llm_wrapper/llm_engine.py`
  - `local-llm-wrapper/local_llm_wrapper/llm_prompts.py`
  - `local-llm-wrapper/local_llm_wrapper/llm_utils.py`
  - `local-llm-wrapper/local_llm_wrapper/transports/apple.py`
  - `local-llm-wrapper/local_llm_wrapper/transports/ollama.py`
- Updated CLI scripts to import transport classes directly from module paths:
  - `local-llm-wrapper/llm_chat.py`
  - `local-llm-wrapper/llm_generate.py`
  - `local-llm-wrapper/llm_xml_demo.py`
- Simplified `local-llm-wrapper/local_llm_wrapper/transports/__init__.py` to satisfy
  `__init__.py` policy checks.
- Hardened `local-llm-wrapper/local_llm_wrapper/transports/ollama.py` endpoint handling:
  added HTTP/HTTPS scheme + host validation and Bandit-safe call annotations for validated
  `urllib.request.urlopen` usage.
- Re-enabled `local-llm-wrapper` coverage in repo hygiene tests by removing it from test skip-dir
  sets.
- Added import-requirements alias mapping:
  `applefoundationmodels -> apple-foundation-models`.

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_bandit_security.py tests/test_import_dot.py tests/test_import_requirements.py tests/test_init_files.py'` (pass: `52 passed`)
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/'` (pass: `322 passed`)

Next actions
1. Keep `local-llm-wrapper` import style absolute to avoid future import-dot regressions.
2. If transport package re-exports are needed later, move them outside `__init__.py` to keep
   init-file policy clean.

---
Session Update: 2026-02-22 (Blog Output Naming + Prompt Quality Hardening)

What changed
- Updated `pipeline/outline_to_blog_post.py` to date-stamp blog output filenames with local date
  (`YYYY-MM-DD`) while avoiding time stamps in filenames.
- Added filename handling so:
  - `out/blog_post.md` becomes `out/blog_post_YYYY-MM-DD.md`,
  - already-dated filenames are preserved without duplicate suffixing.
- Tightened blog prompt to steer toward daily human-readable narrative and explicitly avoid generic
  writing-advice/call-to-action phrasing.
- Removed `llm_global_outline` from blog prompt context to reduce deterministic outline-like output.
- Added blog quality validation in generation flow (H1/title presence, minimum word threshold,
  and LLM error-payload detection) with retry behavior.
- Added clean error handling in `main()` so failed generation logs concise failure messages and exits
  without traceback noise.
- Updated tests in `tests/test_outline_to_blog_post.py` for new prompt constraints and filename
  date-stamp behavior.

What was tested
- `bash -lc 'source source_me.sh && PYTHONPYCACHEPREFIX=/tmp/vosslab_podcast_pycache python3.12 -m pytest -q tests/test_outline_to_blog_post.py tests/test_content_pipeline_limits.py'` (pass: `8 passed`)
- `bash -lc 'source source_me.sh && python3.12 -m py_compile pipeline/outline_to_blog_post.py'`
- `bash -lc 'source source_me.sh && python3.12 pipeline/outline_to_blog_post.py'` (validated clean failure path without traceback when Apple generation fails)

Next actions
1. For stable local runs, consider enabling Ollama provider in `settings.yaml` if Apple transport
   intermittently fails on generation.
2. Optionally externalize blog prompt text into `prompts/` templates in a follow-up.

---
Session Update: 2026-02-22 (Blog Word Target Softened)

What changed
- Updated `pipeline/outline_to_blog_post.py` so blog word count is treated strictly as a target.
- Removed minimum-word hard gating from `blog_quality_issue`; short valid Markdown is no longer
  rejected during repo-pass/fallback/final synthesis checks.
- Kept quality checks for structural and error conditions (empty output, model error payload, and
  missing H1 title).
- Updated `tests/test_outline_to_blog_post.py` with coverage that short valid Markdown passes
  quality checks.
- Updated `docs/CHANGELOG.md` to reflect target-only word behavior.

What was tested
- `pytest -q tests/test_outline_to_blog_post.py tests/test_content_pipeline_limits.py` (pass:
  `10 passed`)

Next actions
1. Run `python3 pipeline/outline_to_blog_post.py` on your latest outline and tune prompt style as
   needed for more human voice.
2. If desired, apply the same target-only treatment to podcast-script generation prompts.

---
Session Update: 2026-02-22 (Salvage-First Blog Markdown Normalization)

What changed
- Reviewed archived guardrail plan from `automated_radio_disc_jockey` and adopted one tolerance
  pattern for this repo: salvage usable output before retrying/rejecting for structure.
- Updated `pipeline/outline_to_blog_post.py` with `normalize_markdown_blog()`:
  - promotes leading `##` to `#`,
  - injects default `# Daily Engineering Update` when no H1 exists.
- Blog generation flow now normalizes repo drafts, fallback output, and final synthesis output
  before quality checks.
- `blog_quality_issue` now focuses on unusable/error outputs and no longer blocks solely for
  missing H1 shape.
- Added tests in `tests/test_outline_to_blog_post.py` for H1 salvage behavior.

What was tested
- `pytest -q tests/test_outline_to_blog_post.py tests/test_content_pipeline_limits.py` (pass:
  `12 passed`)

Next actions
1. If desired, externalize blog prompts into editable files under a `prompts/` directory next,
   following the same pattern you referenced.

---
Session Update: 2026-02-22 (Repo-by-Repo Blog Draft + Final Trim Flow)

What changed
- Updated `pipeline/outline_to_blog_post.py` generation flow to match requested strategy:
  1) generate one draft per repo using target
     `max(100, ceil((2*word_limit)/(N-1)))`,
  2) pick the single best repo draft,
  3) run one final LLM call to pare that draft to the target blog length.
- Added explicit stage logging for repo draft generation, retry/skip decisions, best-draft
  selection, and final trim pass.
- Added configured LLM execution-path logging (for example
  `apple(local foundation models) -> ollama(model=...)`) at run start.
- Enabled local-llm-wrapper runtime transport logs for this script (`quiet=False`) so each call
  shows which transport is queried.

What was tested
- `pytest -q tests/test_outline_to_blog_post.py tests/test_content_pipeline_limits.py` (pass:
  `12 passed`)
- `pytest -q tests/` (pass: `329 passed`)

Next actions
1. Run one local blog generation and confirm logs now clearly distinguish incremental draft passes
   from the final trim pass.

---
Session Update: 2026-02-22 (LLM Feature Parity + Repo Draft Cache for Outline-to Stages)

What changed
- Added shared LLM helper module `pipeline/podlib/outline_llm.py` to reduce duplicate setup code
  across `outline_to_*` scripts:
  - local-llm-wrapper path resolution
  - configured execution-path description
  - transport client creation
  - incremental target formula helper
- Added reusable draft-cache helper module `pipeline/podlib/outline_draft_cache.py`.
- Updated `pipeline/outline_to_blog_post.py` to support per-repo intermediate draft cache reuse
  with `--continue`/`--no-continue` (default continue) and `--repo-draft-cache-dir`.
- Reworked `pipeline/outline_to_bluesky_post.py` to LLM orchestration parity:
  - per-repo incremental drafts
  - best-draft selection
  - final trim pass
  - settings-driven transport/model/token config
  - explicit stage and execution-path logging
  - per-repo draft cache reuse flags (`--continue`, `--repo-draft-cache-dir`)
- Reworked `pipeline/outline_to_podcast_script.py` to LLM orchestration parity:
  - per-repo incremental drafts
  - best-draft selection
  - final trim pass
  - settings-driven transport/model/token config
  - explicit stage and execution-path logging
  - per-repo draft cache reuse flags (`--continue`, `--repo-draft-cache-dir`)
  - speaker-line salvage and required-speaker coverage fallback
- Added tests:
  - `tests/test_outline_to_bluesky_post.py`
  - `tests/test_outline_to_podcast_script.py`
- Updated `README.md` and `docs/CHANGELOG.md` for new LLM + cache behavior.
- Removed generated `__pycache__` directories from working tree after test runs.

What was tested
- `pytest -q tests/test_outline_to_blog_post.py tests/test_outline_to_bluesky_post.py tests/test_outline_to_podcast_script.py tests/test_content_pipeline_limits.py` (pass: `18 passed`)
- `pytest -q tests/` (pass: `340 passed`)

Next actions
1. Run one real local pass of `outline_to_bluesky_post.py` and `outline_to_podcast_script.py` to
   confirm cache-hit logs appear on immediate rerun.
2. Optionally externalize Bluesky/podcast prompts to a `prompts/` folder for human editing parity
   with your other repo pattern.

---
Session Update: 2026-02-22 (run_local_pipeline Rule Alignment)

What changed
- Updated `automation/run_local_pipeline.sh` to align with repo rule docs:
  - uses `git rev-parse --show-toplevel` for `REPO_ROOT`,
  - requires and sources `source_me.sh`,
  - runs stage commands via `python3` after sourcing (instead of hardcoding `.venv/bin/python`),
  - uses daily fetch preset (`--last-day`),
  - writes blog output as Markdown (`out/blog_post.md`) to match current pipeline behavior.

What was tested
- `bash -n automation/run_local_pipeline.sh`
- `source source_me.sh && pytest tests/test_shebangs.py tests/test_ascii_compliance.py -q` (pass: `2 passed`)

Next actions
1. If desired, add a `--last-week` option to `automation/run_local_pipeline.sh` for manual weekly override while keeping daily default.

---
Session Update: 2026-02-22 (Python Local Runner + Fetch Date Files + 403 Tolerance)

What changed
- Replaced shell runner with `automation/run_local_pipeline.py` (Rich-colored logs, stage timing summary, per-stage retry handling).
- Removed legacy `automation/run_local_pipeline.sh` and updated `automation/install_launchd_pipeline.sh` to run the Python runner with `python3`.
- Updated `README.md` launchd runner references to `automation/run_local_pipeline.py`.
- Added `rich` to `pip_requirements.txt` for improved terminal output in the Python runner.
- Updated `pipeline/fetch_github_data.py` to date-stamp top-level JSONL output filenames by default (for example `out/github_data_YYYY-MM-DD.jsonl`).
- Updated `pipeline/podlib/github_client.py` to add request jitter (`time.sleep(random.random())`) and one 10-second retry on 403 responses before raising.

What was tested
- `source source_me.sh && pytest tests/` (pass: `357 passed`)

Next actions
1. Run `python3 automation/run_local_pipeline.py --last-day` locally and confirm stage logs/readability match your preference.
2. If desired, add CLI flags to the Python runner for overriding blog/bluesky/podcast target lengths without editing code.

---
Session Update: 2026-02-21 (Fail-Fast for outline_to_* stages)

What changed
- Added fail-fast activity guards to `pipeline/outline_to_bluesky_post.py` and `pipeline/outline_to_podcast_script.py`.
- Both scripts now check outline totals immediately after loading input and exit early when there is no repo commit activity (`totals.repos == 0` or `totals.commit_records == 0`).
- Early exit happens before any LLM call and before writing output files.
- Logging now explicitly reports no-activity exits:
  - Bluesky: "No repo commit activity in outline; exiting bluesky stage without LLM calls." and "No Bluesky file written."
  - Podcast: "No repo commit activity in outline; exiting podcast script stage without LLM calls." and "No podcast script file written."
- `pipeline/outline_to_blog_post.py` already had matching fail-fast behavior and was left as-is.

What was tested
- `source source_me.sh && pytest tests/` (executed via `bash -lc ...`; pass: `369 passed`)

Next actions
1. Optionally mirror the same early-exit policy in any future `outline_to_*` stages to keep behavior consistent.

---
Session Update: 2026-02-21 (fetch last complete day at 5am local)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/fetch_github_data.py` so `--last-day` (and other N-day windows) use the last fully completed local reset window at 05:00.
- Added `compute_completed_window_local()` and `compute_completed_window_utc()` to centralize window boundary logic.
- Active fetch windows now anchor at local 05:00, then convert to UTC for API calls and record timestamps.
- Output filename date stamp now uses local window start date (completed-day label), not wall-clock current date.
- Daily cache day keys now derive from window start, and timestamp bucketing now uses local logical-day mapping with the 05:00 reset boundary.
- `parse_iso()` now normalizes naive timestamps to UTC to keep datetime handling safe.

What was tested
- `source source_me.sh && pytest tests/test_fetch_github_data_features.py -q` (pass: `10 passed`)
- `source source_me.sh && pytest tests/ -q` (pass: `371 passed`)

Next actions
1. If you want the reset hour configurable, add `github.day_reset_hour_local` in `settings.yaml` and thread it through fetch + runner logs.

---
Session Update: 2026-02-21 (TZ-aware 5am reset window)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/fetch_github_data.py` to make 5am reset-window timezone explicit.
- Reset timezone now resolves from `TZ` env first; if unset, defaults to `America/Chicago`.
- Window boundaries, day-key generation, and per-record daily bucket mapping now all use the same resolved reset timezone.
- Log output now prints reset timezone name in the window line.

What was tested
- `source source_me.sh && pytest tests/test_fetch_github_data_features.py -q` (pass: `10 passed`)
- `source source_me.sh && pytest tests/ -q` (pass: `371 passed`)

Next actions
1. If desired, add `github.day_reset_timezone` in `settings.yaml` so timezone can be set without environment variables.

---
Session Update: 2026-02-21 (User-scoped default output paths)

What changed
- Added shared helpers in `/Users/vosslab/nsh/vosslab_podcast/pipeline/podlib/pipeline_settings.py`:
  - `get_github_username()`
  - `resolve_user_scoped_out_path()`
- Updated pipeline scripts so default `out/...` paths now resolve to `out/<github.username>/...` while preserving explicit custom CLI paths:
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/fetch_github_data.py`
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/github_data_to_outline.py`
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_to_blog_post.py`
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_to_bluesky_post.py`
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_to_podcast_script.py`
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/script_to_audio.py`
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/script_to_audio_say.py`
- `fetch_github_data.py` repo-list cache path is now user-scoped by default (`out/<user>/cache/list_repos.json`).
- `github_data_to_outline.py` now falls back to latest `out/<user>/github_data_*.jsonl` when default input path is missing.
- Updated `/Users/vosslab/nsh/vosslab_podcast/automation/run_local_pipeline.py` to rely on script defaults (user-scoped paths), load username from settings, and detect latest user-scoped fetch output for post-fetch summary checks.

What was tested
- `source source_me.sh && pytest tests/` (pass: `371 passed`)

Next actions
1. If you want strict per-user separation for low-level GitHub API cache files too, add an explicit user-scoped cache root to `GitHubClient` initialization and pass it from fetch.

---
Session Update: 2026-02-21 (OUT directory spec + full user-scoped cache alignment)

What changed
- Added `/Users/vosslab/nsh/vosslab_podcast/docs/OUT_DIRECTORY_ORGANIZATION_SPEC.md` as the canonical output-layout contract for all scripts.
- Linked spec from `/Users/vosslab/nsh/vosslab_podcast/README.md`.
- Completed user-scoped separation for low-level GitHub API cache:
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/podlib/github_client.py` now accepts `cache_dir` in `GitHubClient`.
  - `/Users/vosslab/nsh/vosslab_podcast/pipeline/fetch_github_data.py` now passes user-scoped `out/<user>/cache/github_api`.

What was tested
- `source source_me.sh && pytest tests/` (pass: `371 passed`)

Next actions
1. Optionally update README examples to use current script names and user-scoped default paths throughout.

---
Session Update: 2026-02-21 (OUT spec compliance audit for scripts)

What changed
- Audited `pipeline/` and `automation/` scripts against `/Users/vosslab/nsh/vosslab_podcast/docs/OUT_DIRECTORY_ORGANIZATION_SPEC.md`.
- Fixed remaining non-compliant output path in `/Users/vosslab/nsh/vosslab_podcast/automation/install_launchd_pipeline.sh`:
  - moved launchd logs from bare `out/` root to allowed scratch namespace `out/tmp/launchd/`.
- Verified script defaults remain user-scoped in pipeline stages (`out/<user>/...`) with CLI override preservation.

What was tested
- `bash -n automation/install_launchd_pipeline.sh`
- `bash -n automation/uninstall_launchd_pipeline.sh`
- `source source_me.sh && pytest tests/` (pass: `371 passed`)

Next actions
1. Optionally add a dedicated compliance test that rejects new bare `out/` writes in scripts (except approved namespaces like `out/tmp/`, `out/smoke/`, `out/samples/`, `out/archive/`).

---
Session Update: 2026-02-21 (Logs namespace update)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/docs/OUT_DIRECTORY_ORGANIZATION_SPEC.md` to define logs namespace:
  - `out/logs/<program>/...`
  - includes `out/logs/launchd/...` examples.
- Updated `/Users/vosslab/nsh/vosslab_podcast/automation/install_launchd_pipeline.sh` to write logs to:
  - `out/logs/launchd/launchd_pipeline.log`
  - `out/logs/launchd/launchd_pipeline.error.log`

What was tested
- `bash -n automation/install_launchd_pipeline.sh`
- `bash -n automation/uninstall_launchd_pipeline.sh`
- `source source_me.sh && pytest tests/` (pass: `371 passed`)

Next actions
1. Optionally add a small helper in `podlib` for script-level default log directories (`out/logs/<script_name>/`) to standardize future logging behavior.

---
Session Update: 2026-02-21 (Launchd system log path exception)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/automation/install_launchd_pipeline.sh` to write logs to system log path:
  - `~/Library/Logs/vosslab_podcast/launchd/launchd_pipeline.log`
  - `~/Library/Logs/vosslab_podcast/launchd/launchd_pipeline.error.log`
- Updated `/Users/vosslab/nsh/vosslab_podcast/docs/OUT_DIRECTORY_ORGANIZATION_SPEC.md` with an explicit launchd exception allowing `~/Library/Logs/...`.
- Updated spec log examples to non-launchd `out/logs/<program>/...` paths to avoid ambiguity.

What was tested
- `bash -n automation/install_launchd_pipeline.sh`
- `bash -n automation/uninstall_launchd_pipeline.sh`
- `source source_me.sh && pytest tests/` (pass: `371 passed`)

Next actions
1. Optionally add a tiny smoke check in install script to print absolute log folder path after install for quick manual verification.

---
Session Update: 2026-02-21 (Blog voice + dated artifacts + outline compilation stage)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_to_blog_post.py` prompts to require first-person singular narrative voice.
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_to_bluesky_post.py` to date-stamp output filename by default (`bluesky_post-YYYY-MM-DD.txt`).
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_to_podcast_script.py` to date-stamp output filename by default (`podcast_script-YYYY-MM-DD.txt`).
- Updated `/Users/vosslab/nsh/vosslab_podcast/automation/run_local_pipeline.py`:
  - added required `outline_compilation` stage,
  - added end-of-run artifact list output,
  - artifact list includes expected mp3 path (`podcast_audio-YYYY-MM-DD.mp3`) with not-generated status when absent.
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/github_data_to_outline.py`:
  - markdown-oriented rendering (`# GitHub Daily Outline`),
  - default outline markdown path now `out/.../outline.md`,
  - writes dated daily snapshots to `out/<user>/daily_outlines/github_outline-YYYY-MM-DD.json|md`.
- Added new script `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_compilation.py`:
  - compiles daily outlines for `--last-day|--last-week|--last-month`,
  - skips daily outlines with zero repo/commit activity,
  - writes compiled JSON for downstream (`out/<user>/outline.json`) and dated markdown (`compilation_outline-<window>-YYYY-MM-DD.md`).
- Updated `/Users/vosslab/nsh/vosslab_podcast/README.md` process flow and output naming to reflect current pipeline scripts and dated artifacts.
- Updated `/Users/vosslab/nsh/vosslab_podcast/tests/test_outline_parser.py` expectations for markdown daily-outline headings.

What was tested
- `source source_me.sh && pytest tests/` (pass: `372 passed`)

Next actions
1. If you want strict day-by-day generation from one multi-day fetch input, wire `run_local_pipeline.py` to iterate `out/<user>/daily_cache/*.jsonl` through `github_data_to_outline.py` before compilation.

---
Session Update: 2026-02-21 (Daily outline word targets)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/github_data_to_outline.py` word-target behavior:
  - global daily outline target set to ~2000 words with retry band 1000-4000 words,
  - per-repo target now computed by formula `max(750, ceil(2000/(N-1)))` where `N` is selected repo count,
  - removed manual `--llm-repo-target-words` override so formula behavior is consistent,
  - added prompt guidance and retry logs for off-target repo/global lengths.
- Adjusted parser test expectations for optional retry purpose labels in
  `/Users/vosslab/nsh/vosslab_podcast/tests/test_outline_parser.py`.
- Marked `/Users/vosslab/nsh/vosslab_podcast/pipeline/outline_compilation.py` executable for shebang policy compliance.

What was tested
- `source source_me.sh && pytest tests/test_outline_parser.py -q` (pass)
- `source source_me.sh && pytest tests/test_shebangs.py -q` (pass)
- `source source_me.sh && pytest tests/ -q` (pass: `377 passed`)

Next actions
1. If desired, expose daily/global target words in settings.yaml while keeping the formula as default behavior.

---
Session Update: 2026-02-21 (--no-api-calls local runner mode)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/automation/run_local_pipeline.py` with `--no-api-calls` mode.
- In this mode, fetch stage is skipped and the runner reuses latest cached fetch JSONL from user-scoped output paths, avoiding GitHub API calls while testing downstream stages.
- Added fail-fast error when `--no-api-calls` is requested but no cached fetch file exists.
- Updated `/Users/vosslab/nsh/vosslab_podcast/docs/CHANGELOG.md` with this behavior change and validation note.

What was tested
- `source source_me.sh && pytest tests/ -q` (pass: `378 passed`)

Next actions
1. Optionally add a short README usage example for `automation/run_local_pipeline.py --no-api-calls`.

---
Session Update: 2026-02-22 (Date-stamped podcast audio artifact)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/script_to_audio_say.py` to date-stamp output audio filenames by default using local date (`podcast_audio-YYYY-MM-DD.mp3`).
- Added local date-stamp helpers in that script and applied them before output path resolution.
- Updated `/Users/vosslab/nsh/vosslab_podcast/automation/run_local_pipeline.py` artifact detection to look for `podcast_audio-*.mp3`.
- Updated `/Users/vosslab/nsh/vosslab_podcast/docs/CHANGELOG.md` to record this behavior change.

What was tested
- `source source_me.sh && pytest -q tests/test_script_to_audio_say.py` (pass: `5 passed`)
- `source source_me.sh && pytest -q tests/test_content_pipeline_limits.py tests/test_pipeline_settings.py` (pass: `13 passed`)

Next actions
1. Optionally update README examples that still mention non-dated audio output names.

---
Session Update: 2026-02-22 (Audio output naming split)

What changed
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/script_to_audio.py` so default output is date-stamped `podcast_audio-YYYY-MM-DD.mp3`.
- Updated `/Users/vosslab/nsh/vosslab_podcast/pipeline/script_to_audio_say.py` so default output is date-stamped `narrator_audio-YYYY-MM-DD.mp3`.
- Updated `/Users/vosslab/nsh/vosslab_podcast/automation/run_local_pipeline.py` artifact list to include both `podcast_audio-*.mp3` and `narrator_audio-*.mp3`.
- Updated `/Users/vosslab/nsh/vosslab_podcast/docs/CHANGELOG.md` with the audio output naming behavior change.

What was tested
- `source source_me.sh && pytest tests/` (pass: `425 passed`)

Next actions
1. Optionally refresh README examples that still use legacy audio filenames such as `episode.wav` or `episode_siri.aiff`.
